[{"authors":["ahme0307"],"categories":null,"content":"  I am a Research scientist at Sintef Digital and an Adjunct associate professor position at NTNU. I did my Ph.D. at the department of computer science at the Norwegian University of Science and Technology, advised by Sule Yildirim Yayilgan, Marius Pedersen, and Øistein Hovde. Before that, I received a Master\u0026rsquo;s degree from Chonbuk National University (Jeonju, South Korea) in 2014 and a Bachelor\u0026rsquo;s degree from Addis Ababa University (Addis Ababa, Ethiopia) in 2009.\nMy research interests are broadly in Machine learning and computer vision, with emphasis on 3D vision and medical imaging for explainable and data-efficient learning. My current work focuses on deep learning for point cloud processing, underwater navigation, and capsule video endoscopy.\nWould you be interested in collaborating? then drop me an email or connect on LinkedIn\n Below are sample projects I have worked on:\n","date":1637625600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1637625600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"//localhost:1313/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a Research scientist at Sintef Digital and an Adjunct associate professor position at NTNU. I did my Ph.D. at the department of computer science at the Norwegian University of Science and Technology, advised by Sule Yildirim Yayilgan, Marius Pedersen, and Øistein Hovde. Before that, I received a Master\u0026rsquo;s degree from Chonbuk National University (Jeonju, South Korea) in 2014 and a Bachelor\u0026rsquo;s degree from Addis Ababa University (Addis Ababa, Ethiopia) in 2009.","tags":null,"title":"Ahmed Mohammed","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026#34;Courses\u0026#34; url = \u0026#34;courses/\u0026#34; weight = 50 Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026#34;Docs\u0026#34; url = \u0026#34;docs/\u0026#34; weight = 50 Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"//localhost:1313/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"//localhost:1313/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"//localhost:1313/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"//localhost:1313/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":["Anuja Vats","Marius Pedersen","Ahmed Mohammed","Øistein Hovde"],"categories":null,"content":"","date":1637625600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637625600,"objectID":"d2ec038d3ee24bb72700ff56fb30514d","permalink":"//localhost:1313/publication/vats2021learning/","publishdate":"2021-11-23T00:00:00Z","relpermalink":"/publication/vats2021learning/","section":"publication","summary":"The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy (WCE) is thwarted by the lack of data. The inadequacy in richly representative healthy and abnormal conditions results in isolated analyses of pathologies, that can not handle realistic multi-pathology scenarios. In this work, we explore how to learn more for free, from limited data through solving a WCE multicentric, multi-pathology classification problem. Learning more implies to learning more than full supervision would allow with the same data. This is done by combining self supervision with full supervision, under multi task learning. Additionally, we draw inspiration from the Human Visual System (HVS) in designing self supervision tasks and investigate if seemingly ineffectual signals within the data itself can be exploited to gain performance, if so, which signals would be better than others. Further, we present our analysis of the high level features as a stepping stone towards more robust multi-pathology CADx in WCE. Code accompanying this work will be made available on github.","tags":null,"title":"Learning More for Free - A Multi Task Learning Approach for Improved Pathology Classification in Capsule Endoscopy (MICCAI 2021)","type":"publication"},{"authors":["Petter Risholm","Peter Ørnulf Ivarsen","Karl Henrik Haugholt","Ahmed Mohammed"],"categories":null,"content":"","date":1634947200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634947200,"objectID":"6331b902bb8c6352f07a5ceb5629274b","permalink":"//localhost:1313/publication/risholm2021underwater/","publishdate":"2021-10-23T00:00:00Z","relpermalink":"/publication/risholm2021underwater/","section":"publication","summary":"We propose a system for 6-DoF estimation of Aruco markers with associated uncertainties in the challenging underwater environment. A state-of-the-art object detection framework (EfficientDet) was adapted to predict the corner locations of Aruco markers, while dropout sampling at inference time is used to estimate the predictive 6-DoF pose uncertainty. A dataset of Aruco markers captured in a wide variety of turbidities, with ground truth position of the corner locations, was gathered and used to train the network to robustly predict the 6-DoF pose. We report median translational errors of 2.6cm at low turbidity (8.5m attenuation length) and up to 10.5cm at high turbidities (0.3m attenuation length). The respective uncertainty, reported as interquartile ranges (IQRs), range from 3.2cm up to 27.9cm. The rotational median errors varied from 5.6 (deg) to 10.7 (deg) with IQRs of 6.4 (deg) to 26.2 (deg). We also discuss how the pose uncertainty can be applied to reduce the risk in a subsea intervention operation.","tags":null,"title":"Underwater marker-based pose-estimation with associated uncertainty (ICCVw2021)","type":"publication"},{"authors":["Ahmed Mohammed","Johannes Kvam","Jens T. Thielemann","Karl H. Haugholt","Petter Risholm"],"categories":null,"content":"","date":1632355200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632355200,"objectID":"85b332cfd221a9f5f5669bee5ed74ef1","permalink":"//localhost:1313/publication/mohammed20216d/","publishdate":"2021-09-23T00:00:00Z","relpermalink":"/publication/mohammed20216d/","section":"publication","summary":"Manipulation tasks on subsea instalments require extremely precise detection and localization of objects of interest. This problem is referred to as “pose estimation”. In this work, we present a framework for detecting and predicting 6DoF pose for relevant objects (fish-tail, gauges, and valves) on a subsea panel under varying water turbidity. A deep learning model that takes 3D vision data as an input is developed, providing a more robust 6D pose estimate. Compared to the 2D vision deep learning model, the proposed method reduces rotation and translation prediction error by (−Δ0.39∘) and translation (−Δ6.5 mm), respectively, in high turbid waters. The proposed approach is able to provide object detection as well as 6D pose estimation with an average precision of 91%. The 6D pose estimation results show 2.59∘ and 6.49 cm total average deviation in rotation and translation as compared to the ground truth data on varying unseen turbidity levels. Furthermore, our approach runs at over 16 frames per second and does not require pose refinement steps. Finally, to facilitate the training of such model we also collected and automatically annotated a new underwater 6D pose estimation dataset spanning seven levels of turbidity.","tags":null,"title":"6D Pose Estimation for Subsea Intervention in Turbid Waters (Electronics)","type":"publication"},{"authors":["Marius Pedersen","Ahmed Mohammed"],"categories":null,"content":"","date":1632096000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1632096000,"objectID":"95ab4e4014e99136052725ca5dd615e0","permalink":"//localhost:1313/publication/pedersen2021photo/","publishdate":"2021-09-20T00:00:00Z","relpermalink":"/publication/pedersen2021photo/","section":"publication","summary":"Individual fish identification and recognition is an important step in the conservation and management of fisheries. One of most frequently used methods involves capturing and tagging fish. However, these processes have been reported to cause tissue damage, premature tag loss, and decreased swimming capacity. More recently, marine video recordings have been extensively used for monitoring fish populations. However, these require visual inspection to identify individual fish. In this work, we proposed an automatic method for the identification of individual brown trouts, Salmo trutta. We developed a deep convolutional architecture for this purpose. Specifically, given two fish images, multi-scale convolutional features were extracted to capture low-level features and high-level semantic components for embedding space representation. The extracted features were compared at each scale for capturing representation for individual fish identification. The method was evaluated on a dataset called NINA204 based on 204 videos of brown trout and on a dataset TROUT39 containing 39 brown trouts in 288 frames. The identification method distinguished individual fish with 94.6% precision and 74.3% recall on a NINA204 video sequence with significant appearance and shape variation. The identification method takes individual fish and is able to distinguish them with precision and recall percentages of 94.6% and 74.3% on NINA204 for a video sequence with significant appearance and shape variation.","tags":null,"title":"Photo Identification of Individual Salmo trutta Based on Deep Learning (Applied Sciences)","type":"publication"},{"authors":["Ahmed Mohammed","Ivar Farup","Marius Pedersen","Sule Yildirim","Øistein Hovde"],"categories":null,"content":"","date":1608508800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608508800,"objectID":"69df7edeb0ff9f00d8ae8d63c89ecfad","permalink":"//localhost:1313/publication/mohammed2020ps/","publishdate":"2020-12-21T00:00:00Z","relpermalink":"/publication/mohammed2020ps/","section":"publication","summary":"We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for frame-level anomaly detection and multi-label classification of different colon diseases in video capsule endoscopy (VCE) data. Our proposed model is capable of coping with the key challenge of colon apparent heterogeneity caused by several types of diseases. Our model is driven by attention-based deep multiple instance learning and is trained end-to-end on weakly labeled data using video labels instead of detailed frame-by-frame annotation. This makes it a cost-effective approach for the analysis of large capsule video endoscopy repositories. Other advantages of our proposed model include its capability to localize gastrointestinal anomalies in the temporal domain within the video frames, and its generality, in the sense that abnormal frame detection is based on automatically derived image features. The spatial and temporal features are obtained through ResNet50 and residual Long short-term memory (residual LSTM) blocks, respectively. Additionally, the learned temporal attention module provides the importance of each frame to the final label prediction. Moreover, we developed a self-supervision method to maximize the distance between classes of pathologies. We demonstrate through qualitative and quantitative experiments that our proposed weakly supervised learning model gives a superior precision and F1-score reaching, 61.6% and 55.1%, as compared to three state-of-the-art video analysis methods respectively. We also show our model’s ability to temporally localize frames with pathologies, without frame annotation information during training. Furthermore, we collected and annotated the first and largest VCE dataset with only video labels. The dataset contains 455 short video segments with 28,304 frames and 14 classes of colorectal diseases and artifacts. Dataset and code supporting this publication will be made available on our home page.","tags":null,"title":"PS-DeVCEM: Pathology-sensitive deep learning model for video capsule endoscopy based on weakly labeled data (Computer Vision and Image Understanding)","type":"publication"},{"authors":["Ahmed Mohammed","Congcong Wang","Meng Zhao","Mohib Ullah","Rabia Naseem","Hao Wang","Marius Pedersen-","Faouzi Alaya Cheikh"],"categories":null,"content":"","date":1597968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1597968000,"objectID":"d05857dcc058cb1ee7057af0b84a58d5","permalink":"//localhost:1313/publication/mohammed2020weakly/","publishdate":"2020-08-21T00:00:00Z","relpermalink":"/publication/mohammed2020weakly/","section":"publication","summary":"Deep Learning-based chest Computed Tomography (CT) analysis has been proven to be effective and efficient for COVID-19 diagnosis. Existing deep learning approaches heavily rely on large labeled data sets, which are difficult to acquire in this pandemic situation. Therefore, weakly-supervised approaches are in demand. In this paper, we propose an end-to-end weakly-supervised COVID-19 detection approach, ResNext+, that only requires volume level data labels and can provide slice level prediction. The proposed approach incorporates a lung segmentation mask as well as spatial and channel attention to extract spatial features. Besides, Long Short Term Memory (LSTM) is utilized to acquire the axial dependency of the slices. Moreover, a slice attention module is applied before the final fully connected layer to generate the slice level prediction without additional supervision. An ablation study is conducted to show the efficiency of the attention blocks and the segmentation mask block. Experimental results, obtained from publicly available datasets, show a precision of 81.9% and F1 score of 81.4%. The closest state-of-the-art gives 76.7% precision and 78.8% F1 score. The 5% improvement in precision and 3% in the F1 score demonstrate the effectiveness of the proposed method. It is worth noticing that, applying image enhancement approaches do not improve the performance of the proposed method, sometimes even harm the scores, although the enhanced images have better perceptual quality.","tags":null,"title":"Weakly-Supervised Network for Detection of COVID-19 in Chest CT Scans (IEEE Access)","type":"publication"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\rPress Space to play!\n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"//localhost:1313/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Ahmed Mohammed","Sule Yildirim Yayilgan","Ivar Farup","Marius Pedersen","Øistein Hovde"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"2f987d9bd40aaa74b8153fffd9fc2925","permalink":"//localhost:1313/publication/spieahmed/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/spieahmed/","section":"publication","summary":"Surgical robot technology has revolutionized surgery toward a safer laparoscopic surgery and ideally been suited for surgeries requiring minimal invasiveness. Sematic segmentation from robot-assisted surgery videos is an essential task in many computer-assisted robotic surgical systems. Some of the applications include instrument detection, tracking and pose estimation. Usually, the left and right frames from the stereoscopic surgical instrument are used for semantic segmentation independently from each other. However, this approach is prone to poor segmentation since the stereo frames are not integrated for accurate estimation of the surgical scene. To cope with this problem, we proposed a multi encoder and single decoder convolutional neural network named StreoScenNet which exploits the left and right frames of the stereoscopic surgical system. The proposed architecture consists of multiple ResNet encoder blocks and a stacked convolutional decoder network connected with a novel sum-skip connection. The input to the network is a set of left and right frames and the output is a mask of the segmented regions for the left frame. It is trained end-to-end and the segmentation is achieved without the need of any pre- or post-processing. We compare the proposed architectures against state-of-the-art fully convolutional networks. We validate our methods using existing benchmark datasets that includes robotic instruments as well as anatomical objects and non-robotic surgical instruments. Compared with the previous instrument segmentation methods, our approach achieves a significant improved Dice similarity coefficient.","tags":null,"title":"StreoScenNet: Surgical Stereo Robotic Scene segmentation (SPIE MI 2019)","type":"publication"},{"authors":["C Wang","Ahmed Mohammed","Faouzi Alaya Cheikh","Azeddine Beghdadi","Ole Jacob Elle"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"b56386f06caaa30416173edcfdd56667","permalink":"//localhost:1313/publication/spiecong/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/spiecong/","section":"publication","summary":"In minimally invasive surgery, smoke generated by such as electrocautery and laser ablation deteriorates image quality severely. This creates discomfortable view for the surgeon which may increase surgical risk and degrade the performance of computer assisted surgery algorithms such as segmentation, reconstruction, tracking, etc. Therefore, real-time smoke removal is required to keep a clear field of view. In this paper, we propose a real-time smoke removal approach based on Convolutional Neural Network (CNN). An encoder-decoder architecture with Laplacian image pyramid decomposition input strategy is proposed. This is an end-to-end network which takes the smoke image and its Laplacian image pyramid decomposition as inputs, and outputs a smoke free image directly without relying on any physical models or estimation of intermediate parameters. This design can be further embedded to deep learning based follow-up image guided surgery processes such as segmentation and tracking tasks easily. A dataset with synthetic smoke images generated from Blender and Adobe Photoshop is employed for training the network. The result is evaluated quantitatively on synthetic images and qualitatively on a laparoscopic dataset degraded with real smoke. Our proposed method can eliminate smoke effectively while preserving the original colors and reaches 26 fps for a video of size 512 × 512 on our training machine. The obtained results not only demonstrate the efficiency and effectiveness of the proposed CNN structure, but also prove the potency of training the network on synthetic dataset.","tags":null,"title":"Multiscale deep desmoking for laparoscopic surgery (SPIE MI 2019)","type":"publication"},{"authors":["Ahmed Mohammed","Sule Yildirim Yayilgan","Ivar Farup","Marius Pedersen","Øistein Hovde"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"9549d989b8c306a065d991649dc357bd","permalink":"//localhost:1313/publication/ynet/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/ynet/","section":"publication","summary":"Colorectal polyps are important precursors to colon cancer, the third most common cause of cancer mortality for both men and women. It is a disease where early detection is of crucial importance. Colonoscopy is commonly used for early detection of cancer and precancerous pathology. It is a demanding procedure requiring a significant amount of time from specialized physicians and nurses, in addition to a significant miss-rates of polyps by specialists. Automated polyp detection in colonoscopy videos has been demonstrated to be a promising way to handle this problem. However, polyps detection is a challenging problem due to the availability of limited amount of training data and large appearance variations of polyps. To handle this problem, we propose a novel deep learning method Y-Net that consists of two encoder networks with a decoder network. Our proposed Y-Net method relies on efficient use of pre-trained and un-trained models with novel sum-skip-concatenation operations. Each of the encoders are trained with encoder specific learning rate along the decoder. Compared with the previous methods employing hand-crafted features or 2-D/3-D convolutional neural network, our approach outperforms state-of-the-art methods for polyp detection with 7.3% F1-score and 13% recall improvement.","tags":null,"title":"Y-Net: A deep Convolutional Neural Network for Polyp Detection (BMVC 2018)","type":"publication"},{"authors":["Ahmed Mohammed","Ivar Farup","Marius Pedersen","Øistein Hovde","Sule Yildirim Yayilgan"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"56a255f7c56987531a53ec1dc65f0020","permalink":"//localhost:1313/publication/cce/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/cce/","section":"publication","summary":"Capsule endoscopy, which uses a wireless camera to take images of the digestive tract, is emerging as an alternative to traditional colonoscopy. The diagnostic values of these images depend on the quality of revealed underlying tissue surfaces. In this paper, we consider the problem of enhancing the visibility of detail and shadowed tissue surfaces for capsule endoscopy images. Using concentric circles at each pixel for random walks combined with stochastic sampling, the proposed method enhances the details of vessel and tissue surfaces. The framework decomposes the image into two detailed layers that contain shadowed tissue surfaces and detail features. The target pixel value is recalculated for the smooth layer using similarity of the target pixel to neighboring pixels by weighting against the total gradient variation and intensity differences. In order to evaluate the diagnostic image quality of the proposed method, we used clinical subjective evaluation with a rank order on selected KID image database and compared it to state-of-the-art enhancement methods. The result showed that the proposed method provides a better result in terms of diagnostic image quality and objective quality contrast metrics and structural similarity index.","tags":null,"title":"Stochastic Capsule Endoscopy Image Enhancement (JI 2018)","type":"publication"},{"authors":["Ahmed Mohammed"],"categories":[],"content":"We participated in gastrointestinal image analysis (GIANA) challenge for detection of polyps from colonoscopy videos, and vascular lesion and inflammation diseases detection from CVE videos. The challenge is organized along international conference on medical image computing and computer assisted intervention (MICCAI2018). A new set of training dataset is provided and ground truth data for the test dataset was kept with the challenge organizers.\n","date":1537401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"4e12ac0ec9e705a32ff42dfe0dd38cd9","permalink":"//localhost:1313/post/challenge/","publishdate":"2018-09-20T00:00:00Z","relpermalink":"/post/challenge/","section":"post","summary":"GIANA 2018 Sub-challenge on Gastrointestinal Image Analysis took part on September 16th during MICCAI 2018 conference, held in Granada, Spain. We participated in gastrointestinal image analysis (GIANA) challenge for detection of polyps from colonoscopy videos, and vascular lesion and inflammation diseases detection from CVE videos.","tags":["Academic"],"title":"MICCAI2018 GIANA challenge","type":"post"},{"authors":["Ahmed Mohammed","Marius Pedersen","Øistein Hovde","Sule Yildirim Yayilgan"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"36b669cd561bd9f87fd55ae10d64b7eb","permalink":"//localhost:1313/publication/cic1/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/cic1/","section":"publication","summary":"This paper proposes a unified framework for capsule video endoscopy image enhancement with an objective to enhance the diagnostic values of these images. The proposed method is based on a hybrid approach of deep learning and classical image processing techniques. Given an input image, it is decomposed spatially into multilayer features. We estimate the base layer with pretrained deep edge aware filters that are learned on the flicker dataset. The detail layers are estimated by the spatio-temporal retinex-inspired envelope with a stochastic sampling technique. The enhanced image is computed by a convex linear combination of the base and the detail layers giving detailed and shadow surface enhanced image. To show its potential, performance comparison between with and without the proposed image enhancement technique is shown using several video images obtained from capsule endoscopy for different parts of digestive organ. Moreover, different learned filters such as Bilateral and L norm are compared for enhancement using an objective image quality metric, BRISQUE, to show the generality of the proposed method.","tags":null,"title":"Deep-STRESS Capsule Video Endoscopy Image Enhancement (CIC 2018)","type":"publication"},{"authors":["Ahmed Mohammed","Ivar Farup","Sule Yildirim Yayilgan","Marius Pedersen","Øistein Hovde"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"211308badc815381094f2ff8d26b5ecf","permalink":"//localhost:1313/publication/tsr/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/tsr/","section":"publication","summary":"Capsule video endoscopy, which uses a wireless camera to visualize the digestive tract, is emerging as an alternative to traditional colonoscopy. Colonoscopy is considered as the gold standard for visualizing the colon and takes 30 frames per second. Capsule images, on the other hand, are taken with low frame rate (average five frames per second), which makes it difficult to find pathology and results in eye fatigue for viewing. In this paper, we propose a variational algorithm to smooth the video temporally and create a visually pleasant video. The main objective of the paper is to increase the frame rate to be closer to that of the colonoscopy. We propose variational energy that takes into consideration both motion estimation and intermediate frame intensity interpolation using the surrounding frames. The proposed formulation incorporates both pixel intensity and texture feature in the optical flow objective function such that the interpolation at the intermediate frame is directly modeled. The main feature of this formulation is that error in motion estimation is incorporated in our model, so that only robust motion estimation are used in estimating the intensity of the intermediate frame. We derived Euler-Lagrange equations and showed an efficient numerical scheme that can be implemented on graphics hardware. Finally, a motion compensated frame rate doubling version of our method is implemented. We evaluate the quality of both 90 and 100% of the frames for medical diagnosis domain through objective image quality metrics. Our method improves state-of-the-art result for 90% frames while performing equivalent for the remaining cases with other existing methods. In the last section, we show application of frame interpolation to informative frame segment visualization and to reduce the power consumption.","tags":null,"title":"Variational Approach for Capsule Video Frame Interpolation (EURASIP JIVP 2018)","type":"publication"},{"authors":["Mohib Ullah","Ahmed Mohammed","Faouzi Cheikh","Zhaohui Wang"],"categories":null,"content":"","date":1512086400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512086400,"objectID":"af239f9f225f06bf79c174fddacee5f2","permalink":"//localhost:1313/publication/mohibicip/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/mohibicip/","section":"publication","summary":"We propose a novel Hierarchical Feature Model (HFM) for multi-target tracking. The traditional tracking algorithms use handcrafted features that cannot track targets accurately when the target model changes due to articulation, illumination intensity variation or perspective distortions. Our HFM explore deep features to model the appearance of targets. Then, we use an unsupervised dimensionality reduction for sparse representation of the feature vectors to cope with the time-critical nature of the tracking problem. Subsequently, a Bayesian filter is adopted as the tracker and a discrete combinatorial optimization is considered for target association. We compare our proposed HFM against 4 state-of-the-art trackers using 4 benchmark datasets. The experimental results show that our HFM outperforms all the state-of-the-art methods in terms of both Multi Object Tracking Accuracy (MOTA) and Multi Object Tracking Precision (MOTP).","tags":null,"title":"A HIERARCHICAL FEATURE MODEL FOR MULTI-TARGET TRACKING (IEEE ICIP 2017)","type":"publication"},{"authors":["Marius Pedersen","Olga Cherepkova","Ahmed Mohammed"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"548b3108ba17f68fef4dc1e84d463591","permalink":"//localhost:1313/publication/olga/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/olga/","section":"publication","summary":"Capsule endoscopy, using a wireless camera to capture the digestive track, is becoming a popular alternative to traditional colonoscopy. The images obtained from a capsule have lower quality compared to traditional colonoscopy, and high-quality images are required by medical doctors in order to set an accurate diagnosis. Over the last years several enhancement techniques have been proposed to improve the quality of capsule images. In order to verify that the capsule images have the required diagnostic quality some kind of quality assessment is required. In this work, the authors evaluate state-of-the-art no-reference image quality metrics for capsule video endoscopy. Furthermore, they use the best performing metric to optimize one of the capsule video endoscopy enhancement methods and validate through subjective experiment.","tags":null,"title":"Image Quality Metrics for the Evaluation and Optimization of Capsule Video EndoscopyEnhancement Techniques (JIST 2017)","type":"publication"},{"authors":["Ahmed Mohammed","Sule Yildirim Yayilgan","Marius Pedersen","Øistein Hovde","Faouzi Cheikh"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"e975721ed589607ae1450c0b2f400a0e","permalink":"//localhost:1313/publication/summary/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/summary/","section":"publication","summary":"Capsule endoscopy, which uses a wireless camera to take images of the digestive track, is emerging as an alternative to traditional wired colonoscopy. A single examination produces a sequence of approximately 50,000 frames. These sequences are manually reviewed, which is time consuming and typically takes about 45–90 minutes and requires the undivided concentration of the reviewer. In this paper, we propose a novel capsule video summarization framework using sparse coding and dictionary learning in feature space. Video frames are clustered into superframes based on power spectral density, and cluster representative frames are used for video summarization. Handcrafted and deep features that are extracted for representative frames are sparse coded using a learned dictionary. Sparse coded features are later used for training SVM classifier. The proposed method was compared with state-of-the-art methods based on sensitivity and specificity. The achieved results show that our proposed framework provides robust capsule video summarization without losing informative segments.","tags":null,"title":"Sparse coded handcrafted and deep features for colon capsule video summarization (IEEE CBMS 2017)","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"//localhost:1313/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"Image Quality enhancement in MEDical diagnosis, monitoring and treatment.","tags":["Demo"],"title":"IQ-MED PROJECT","type":"project"},{"authors":["Ahmed Mohammed"],"categories":[],"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"//localhost:1313/post/jupyter/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"A simple jupyter notebook for MNIST digit classification with CNN","tags":["Academic"],"title":"Introduction to CNN with pytorch","type":"post"},{"authors":["Ahmed Mohammed"],"categories":[],"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"6816afda7390ff5f0fdfec5af7b0b848","permalink":"//localhost:1313/post/imageeval/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/imageeval/","section":"post","summary":"A simple Matlab GUI for rank order image quality evaluation","tags":["Academic"],"title":"Subjective image quality evaluation tool","type":"post"},{"authors":["Ahmed Mohammed"],"categories":[],"content":"","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557619200,"objectID":"d05c1a126ce93b7280e17050e8445838","permalink":"//localhost:1313/post/videoeval/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/videoeval/","section":"post","summary":"A simple Matlab GUI for rank order image quality evaluation","tags":["Academic"],"title":"Video Quality Assessment","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"738713571761b6c4e15c2e3bfa07e679","permalink":"//localhost:1313/project/seavention/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/seavention/","section":"project","summary":"Autonomous subsea intervention - empowered by people and AI.","tags":["Demo"],"title":"SEAVENTION","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"e41214be863f0811345f8e9cc0a5616a","permalink":"//localhost:1313/project/biosys/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/biosys/","section":"project","summary":"SFI Manufacturing's vision is to show that sustainable and advanced production of goods in a high-cost country like Norway is possible, given the right products, the right technologies and the right people involved in the right way.","tags":["Demo"],"title":"SFI Manufacturing","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2d76854d0cb8944b180f14fe4dfa5ab4","permalink":"//localhost:1313/project/sight/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/sight/","section":"project","summary":"One of the biggest challenges in the construction industry is the gap between what is planned and what is actually being built. The current technology for detecting such construction defects is laser scanning, supplemented by manual sampling.","tags":["Demo"],"title":"SIGHT","type":"project"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"162b4a62806a6c609f5c128ad651fd7f","permalink":"//localhost:1313/project/smartfishh2020/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/project/smartfishh2020/","section":"project","summary":"optimize resource efficiency, improve automatic data collection for fish stock assessment, provide evidence of compliance with fishery regulations and reduce the ecological impact of the industry.","tags":["Demo"],"title":"SmartFish","type":"project"}]